{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gg3stai4i24kla35gw5m",
   "authorId": "3690447329246",
   "authorName": "BABU",
   "authorEmail": "babu@squadrondata.com",
   "sessionId": "07f0ac84-2638-44a5-ba14-36cf77856d4f",
   "lastEditTime": 1764946248420
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "#!/usr/bin/env python3\n\"\"\"\nInformatica PowerCenter XML to DBT Project Conversion Script - Production Version v2\nEnhanced with explicit transformation-to-model mapping and improved prompting\nProcesses XML files from Snowflake stage and converts them to DBT projects using Claude-4-Sonnet\n\"\"\"\n\nimport json\nimport re\nimport time\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom functools import wraps\nimport traceback\n\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.cortex import complete\nfrom snowflake.snowpark.functions import col, lit\n\n# ============================================================================\n# CONFIGURATION MANAGEMENT\n# ============================================================================\n\n@dataclass\nclass Config:\n    \"\"\"Centralized configuration for the conversion process\"\"\"\n    # Stage Configuration\n    INPUT_STAGE: str = \"@XML_INPUT\"\n    OUTPUT_STAGE: str = \"@DBT_OUTPUT\"\n    \n    # Model Configuration - UPDATED FOR CLAUDE 4 SONNET\n    #CLAUDE_MODEL: str = \"claude-sonnet-4-5\"\n    CLAUDE_MODEL: str = \"claude-4-sonnet\"    # Claude model for conversion\n    MAX_TOKENS: int = 80000  # Safe limit for Claude 4 Sonnet (200k context)\n    CHUNK_SIZE: int = 75000  # Character limit per chunk\n    \n    # Processing Configuration\n    BATCH_SIZE: int = 5\n    MAX_RETRIES: int = 3\n    RETRY_DELAY: int = 10  # seconds\n    RATE_LIMIT_CALLS_PER_MINUTE: int = 1\n    \n    # Database Configuration\n    OUTPUT_TABLE: str = \"TPC_DI_RAW_DATA.DBT_INGEST.INFORMATICA_DBT_CONVERTED_FILES_MAIN_DEMO\"\n    SUMMARY_TABLE: str = \"TPC_DI_RAW_DATA.DBT_INGEST.INFORMATICA_DBT_CONVERSION_SUMMARY_MAIN_DEMO\"\n    FILE_FORMAT: str = \"xml_format\"\n    \n    # Logging Configuration\n    LOG_LEVEL: str = \"INFO\"\n\n# Initialize global config\nconfig = Config()\n\n# ============================================================================\n# LOGGING SETUP\n# ============================================================================\n\ndef setup_logging(level: str = \"INFO\") -> logging.Logger:\n    \"\"\"Setup structured logging\"\"\"\n    logger = logging.getLogger(\"informatica_dbt_converter\")\n    logger.setLevel(getattr(logging, level))\n    \n    # Clear existing handlers\n    logger.handlers.clear()\n    \n    # Console handler\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    \n    return logger\n\nlogger = setup_logging(config.LOG_LEVEL)\n\n# ============================================================================\n# CUSTOM EXCEPTIONS\n# ============================================================================\n\nclass ConversionError(Exception):\n    \"\"\"Base exception for conversion errors\"\"\"\n    pass\n\nclass XMLReadError(ConversionError):\n    \"\"\"Error reading XML content\"\"\"\n    pass\n\nclass ClaudeAPIError(ConversionError):\n    \"\"\"Error calling Claude API\"\"\"\n    pass\n\nclass ValidationError(ConversionError):\n    \"\"\"Error validating generated content\"\"\"\n    pass\n\n# ============================================================================\n# DECORATORS AND UTILITIES\n# ============================================================================\n\ndef rate_limit(calls_per_minute: int = 1):\n    \"\"\"Rate limiting decorator for API calls\"\"\"\n    min_interval = 60.0 / calls_per_minute\n    last_called = [0.0]\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            elapsed = time.time() - last_called[0]\n            if elapsed < min_interval:\n                sleep_time = min_interval - elapsed\n                logger.debug(f\"Rate limiting: sleeping for {sleep_time:.2f}s\")\n                time.sleep(sleep_time)\n            result = func(*args, **kwargs)\n            last_called[0] = time.time()\n            return result\n        return wrapper\n    return decorator\n\ndef retry_on_failure(max_retries: int = 3, delay: int = 2):\n    \"\"\"Retry decorator with exponential backoff\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_retries - 1:\n                        raise\n                    wait_time = delay * (2 ** attempt)\n                    logger.warning(\n                        f\"Attempt {attempt + 1} failed: {str(e)}. \"\n                        f\"Retrying in {wait_time}s...\"\n                    )\n                    time.sleep(wait_time)\n            return None\n        return wrapper\n    return decorator\n\ndef track_time(operation_name: str):\n    \"\"\"Context manager for tracking operation time\"\"\"\n    class TimeTracker:\n        def __enter__(self):\n            self.start = time.time()\n            return self\n        \n        def __exit__(self, *args):\n            duration = time.time() - self.start\n            logger.info(f\"{operation_name} completed in {duration:.2f}s\")\n    \n    return TimeTracker()\n\n# ============================================================================\n# SNOWFLAKE SESSION MANAGEMENT\n# ============================================================================\n\ndef get_snowflake_session():\n    \"\"\"Initialize Snowflake session with error handling\"\"\"\n    try:\n        session = get_active_session()\n        logger.info(\n            f\"Connected to Snowflake - \"\n            f\"Database: {session.get_current_database()}, \"\n            f\"Schema: {session.get_current_schema()}\"\n        )\n        return session\n    except Exception as e:\n        logger.error(f\"Failed to connect to Snowflake: {e}\")\n        raise\n\n# ============================================================================\n# FILE DISCOVERY AND READING\n# ============================================================================\n\ndef list_xml_files_with_folders(session, stage_name: str) -> Dict[str, List[str]]:\n    \"\"\"List XML files grouped by folder in the specified stage\"\"\"\n    try:\n        with track_time(\"Listing XML files\"):\n            files_result = session.sql(f\"LIST {stage_name}\").collect()\n            \n            folder_files = defaultdict(list)\n            \n            for row in files_result:\n                file_path = row['name']\n                if file_path.lower().endswith('.xml'):\n                    parts = file_path.split('/')\n                    folder_name = parts[1] if len(parts) > 2 else \"root\"\n                    folder_files[folder_name].append(file_path)\n            \n            total_files = sum(len(files) for files in folder_files.values())\n            logger.info(\n                f\"Found {total_files} XML files in {len(folder_files)} folders\")\n            \n            for folder, files in folder_files.items():\n                logger.debug(f\"  Folder '{folder}': {len(files)} files\")\n            \n            return dict(folder_files)\n    \n    except Exception as e:\n        logger.error(f\"Error listing files from {stage_name}: {e}\")\n        raise\n\ndef sanitize_file_path(file_path: str, stage_prefix: str = 'xml_input/') -> str:\n    \"\"\"Sanitize file path by removing stage prefix\"\"\"\n    if file_path.startswith(stage_prefix):\n        return file_path[len(stage_prefix):]\n    return file_path\n\n@retry_on_failure(max_retries=3, delay=2)\ndef read_xml_content(session, stage_name: str, file_path: str) -> Optional[str]:\n    \"\"\"Read XML file content from Snowflake stage with retries\"\"\"\n    try:\n        clean_path = sanitize_file_path(file_path)\n        \n        # Use parameterized approach for safer queries\n        query = f\"SELECT $1 FROM {stage_name}/{clean_path} (FILE_FORMAT => '{config.FILE_FORMAT}')\"\n        \n        logger.debug(f\"Reading file: {clean_path}\")\n        xml_rows = session.sql(query).collect()\n        \n        xml_content = \"\\n\".join([row[0] for row in xml_rows if row[0]])\n        \n        if not xml_content.strip():\n            logger.warning(f\"Empty content for {clean_path}\")\n            raise XMLReadError(f\"Empty content for {clean_path}\")\n        \n        content_size = len(xml_content)\n        estimated_tokens = estimate_token_count(xml_content)\n        logger.info(\n            f\"Read {content_size:,} characters \"\n            f\"(~{estimated_tokens:,} tokens) from {clean_path}\"\n        )\n        \n        return xml_content\n        \n    except Exception as e:\n        logger.error(f\"Error reading {file_path}: {e}\")\n        raise XMLReadError(f\"Failed to read {file_path}: {str(e)}\")\n\n# ============================================================================\n# TOKEN ESTIMATION AND CHUNKING\n# ============================================================================\n\ndef estimate_token_count(text: str) -> int:\n    \"\"\"\n    Improved token count estimation for Claude models\n    Uses a more accurate heuristic based on character types\n    \"\"\"\n    if not text:\n        return 0\n    \n    # Count different character types for better estimation\n    whitespace_count = sum(1 for c in text if c.isspace())\n    alphanumeric_count = sum(1 for c in text if c.isalnum())\n    special_count = len(text) - whitespace_count - alphanumeric_count\n    \n    # Weighted estimation (whitespace tokens are cheaper)\n    estimated = (alphanumeric_count + special_count) / 3.5 + whitespace_count / 6\n    \n    # Add 25% buffer for XML structure overhead\n    return int(estimated * 1.20)\n\ndef extract_xml_sections(xml_content: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    Extract logical sections from XML preserving hierarchical structure\n    Key improvement: Keep MAPPING with all child elements intact\n    \"\"\"\n    sections = []\n    \n    # Strategy: Extract top-level logical units that preserve relationships\n    # Priority order ensures we get complete structures\n    \n    patterns = [\n        # 1. Extract complete MAPPING blocks (contains transformations, instances, connectors)\n        (r'<MAPPING\\s+[^>]*>.*?</MAPPING>', 'MAPPING'),\n        \n        # 2. Extract complete WORKFLOW blocks (contains sessions, tasks)\n        (r'<WORKFLOW\\s+[^>]*>.*?</WORKFLOW>', 'WORKFLOW'),\n        \n        # 3. Extract SOURCE definitions (usually in shared folders)\n        (r'<SOURCE\\s+[^>]*>.*?</SOURCE>', 'SOURCE'),\n        \n        # 4. Extract TARGET definitions (usually in shared folders)\n        (r'<TARGET\\s+[^>]*>.*?</TARGET>', 'TARGET'),\n        \n        # 5. Extract SHORTCUT references (point to shared objects)\n        (r'<SHORTCUT\\s+[^>]*/>|<SHORTCUT\\s+[^>]*>.*?</SHORTCUT>', 'SHORTCUT'),\n    ]\n    \n    # Track extracted positions to avoid duplicates\n    extracted_ranges = []\n    \n    for pattern, section_type in patterns:\n        matches = re.finditer(pattern, xml_content, re.DOTALL)\n        for match in matches:\n            start, end = match.span()\n            \n            # Check if this range overlaps with already extracted sections\n            is_overlap = False\n            for existing_start, existing_end in extracted_ranges:\n                if not (end <= existing_start or start >= existing_end):\n                    is_overlap = True\n                    break\n            \n            if not is_overlap:\n                sections.append((match.group(0), section_type))\n                extracted_ranges.append((start, end))\n                logger.debug(f\"Extracted {section_type} section: {start}-{end} ({end-start} chars)\")\n    \n    if not sections:\n        sections.append((xml_content, 'COMPLETE'))\n        logger.warning(\"No specific sections found, using complete XML\")\n    \n    logger.info(f\"Extracted {len(sections)} sections from XML\")\n    return sections\n\ndef parse_mapping_details(mapping_xml: str) -> Dict[str, Any]:\n    \"\"\"\n    Parse MAPPING XML to extract critical details about data flow\n    This helps Claude understand the transformation logic\n    \"\"\"\n    details = {\n        'mapping_name': '',\n        'transformations': [],\n        'instances': [],\n        'connectors': [],\n        'sources': [],\n        'targets': []\n    }\n    \n    # Extract mapping name\n    mapping_match = re.search(r'<MAPPING\\s+[^>]*NAME\\s*=\\s*\"([^\"]+)\"', mapping_xml)\n    if mapping_match:\n        details['mapping_name'] = mapping_match.group(1)\n    \n    # Extract transformation types and names\n    trans_pattern = r'<TRANSFORMATION\\s+[^>]*NAME\\s*=\\s*\"([^\"]+)\"[^>]*TYPE\\s*=\\s*\"([^\"]+)\"'\n    for match in re.finditer(trans_pattern, mapping_xml):\n        details['transformations'].append({\n            'name': match.group(1),\n            'type': match.group(2)\n        })\n    \n    # Extract instances (actual usage of transformations)\n    inst_pattern = r'<INSTANCE\\s+[^>]*NAME\\s*=\\s*\"([^\"]+)\"[^>]*TRANSFORMATION_NAME\\s*=\\s*\"([^\"]+)\"[^>]*TRANSFORMATION_TYPE\\s*=\\s*\"([^\"]+)\"'\n    for match in re.finditer(inst_pattern, mapping_xml):\n        details['instances'].append({\n            'instance_name': match.group(1),\n            'transformation_name': match.group(2),\n            'transformation_type': match.group(3)\n        })\n    \n    # Extract connectors (data flow) - ALL of them\n    conn_pattern = r'<CONNECTOR\\s+[^>]*FROMINSTANCE\\s*=\\s*\"([^\"]+)\"[^>]*TOINSTANCE\\s*=\\s*\"([^\"]+)\"'\n    for match in re.finditer(conn_pattern, mapping_xml):\n        details['connectors'].append({\n            'from': match.group(1),\n            'to': match.group(2)\n        })\n    \n    return details\n\n\ndef create_chunks(xml_content: str, file_name: str) -> List[Tuple[str, int, int]]:\n    \"\"\"\n    Create intelligent chunks that preserve logical units\n    Key improvement: Keep complete MAPPING and WORKFLOW blocks intact\n    \"\"\"\n    estimated_tokens = estimate_token_count(xml_content)\n    \n    if estimated_tokens < config.MAX_TOKENS:\n        logger.debug(\n            f\"File {file_name} fits in single chunk ({estimated_tokens} tokens)\")\n        return [(xml_content, 1, 1)]\n    \n    logger.info(\n        f\"Large file detected ({estimated_tokens:,} tokens). \"\n        f\"Creating intelligent chunks...\"\n    )\n    \n    # Strategy: Split by major logical units, not by arbitrary sections\n    chunks = []\n    \n    # Extract POWERMART header and REPOSITORY info (keep in first chunk)\n    header_match = re.search(\n        r'(<\\?xml[^>]*>.*?<POWERMART[^>]*>.*?<REPOSITORY[^>]*>)',\n        xml_content,\n        re.DOTALL\n    )\n    header = header_match.group(1) if header_match else \"\"\n    \n    # Extract FOLDER blocks - each folder is a logical unit\n    folder_pattern = r'<FOLDER\\s+[^>]*>.*?</FOLDER>'\n    folders = list(re.finditer(folder_pattern, xml_content, re.DOTALL))\n    \n    if not folders:\n        # No folders found, fall back to section-based chunking\n        logger.warning(\"No FOLDER tags found, using section-based chunking\")\n        return create_chunks_original(xml_content, file_name)\n    \n    logger.info(f\"Found {len(folders)} FOLDER blocks\")\n    \n    # Group folders into chunks based on token limits\n    current_chunk_folders = []\n    current_chunk_tokens = estimate_token_count(header)\n    \n    for folder_match in folders:\n        folder_content = folder_match.group(0)\n        folder_tokens = estimate_token_count(folder_content)\n        \n        # Extract folder name for logging\n        folder_name_match = re.search(r'NAME\\s*=\\s*\"([^\"]+)\"', folder_content)\n        folder_name = folder_name_match.group(1) if folder_name_match else \"unknown\"\n        \n        # Check if adding this folder would exceed token limit\n        if current_chunk_tokens + folder_tokens > config.MAX_TOKENS and current_chunk_folders:\n            # Create chunk from accumulated folders\n            chunk_content = header + \"\\n\" + \"\\n\".join(current_chunk_folders) + \"\\n</REPOSITORY>\\n</POWERMART>\"\n            chunks.append((chunk_content, len(chunks) + 1, 0))\n            logger.info(f\"Created chunk {len(chunks)} with {len(current_chunk_folders)} folders\")\n            \n            # Start new chunk\n            current_chunk_folders = [folder_content]\n            current_chunk_tokens = estimate_token_count(header) + folder_tokens\n        else:\n            # Add folder to current chunk\n            current_chunk_folders.append(folder_content)\n            current_chunk_tokens += folder_tokens\n        \n        logger.debug(f\"Folder '{folder_name}': {folder_tokens:,} tokens\")\n    \n    # Add remaining folders as final chunk\n    if current_chunk_folders:\n        chunk_content = header + \"\\n\" + \"\\n\".join(current_chunk_folders) + \"\\n</REPOSITORY>\\n</POWERMART>\"\n        chunks.append((chunk_content, len(chunks) + 1, 0))\n        logger.info(f\"Created final chunk {len(chunks)} with {len(current_chunk_folders)} folders\")\n    \n    # Handle case where a single folder exceeds token limit\n    if not chunks:\n        logger.warning(f\"Single folder exceeds token limit, splitting by MAPPING blocks\")\n        return split_large_folder(xml_content, file_name)\n    \n    # Update chunk numbers\n    total_chunks = len(chunks)\n    chunks = [(content, idx, total_chunks) for content, idx, _ in chunks]\n    \n    logger.info(f\"Created {total_chunks} folder-based chunks for {file_name}\")\n    return chunks\n\n\ndef split_large_folder(xml_content: str, file_name: str) -> List[Tuple[str, int, int]]:\n    \"\"\"\n    Split a very large folder by MAPPING and WORKFLOW blocks\n    This is a fallback when a single folder exceeds token limits\n    \"\"\"\n    chunks = []\n    \n    # Extract header\n    header_match = re.search(\n        r'(<\\?xml[^>]*>.*?<POWERMART[^>]*>.*?<REPOSITORY[^>]*>.*?<FOLDER[^>]*>)',\n        xml_content,\n        re.DOTALL\n    )\n    header = header_match.group(1) if header_match else \"\"\n    \n    # Extract footer\n    footer = \"</FOLDER>\\n</REPOSITORY>\\n</POWERMART>\"\n    \n    # Extract major blocks (SOURCE, TARGET, MAPPING, WORKFLOW)\n    major_blocks = []\n    \n    # Pattern for major block types\n    block_patterns = [\n        (r'<SOURCE\\s+[^>]*>.*?</SOURCE>', 'SOURCE'),\n        (r'<TARGET\\s+[^>]*>.*?</TARGET>', 'TARGET'),\n        (r'<MAPPING\\s+[^>]*>.*?</MAPPING>', 'MAPPING'),\n        (r'<WORKFLOW\\s+[^>]*>.*?</WORKFLOW>', 'WORKFLOW'),\n        (r'<SHORTCUT\\s+[^>]*/>|<SHORTCUT\\s+[^>]*>.*?</SHORTCUT>', 'SHORTCUT'),\n    ]\n    \n    for pattern, block_type in block_patterns:\n        for match in re.finditer(pattern, xml_content, re.DOTALL):\n            major_blocks.append({\n                'content': match.group(0),\n                'type': block_type,\n                'start': match.start(),\n                'tokens': estimate_token_count(match.group(0))\n            })\n    \n    # Sort by position in document\n    major_blocks.sort(key=lambda x: x['start'])\n    \n    logger.info(f\"Found {len(major_blocks)} major blocks in large folder\")\n    \n    # Group blocks into chunks\n    current_chunk_blocks = []\n    current_chunk_tokens = estimate_token_count(header)\n    \n    for block in major_blocks:\n        if current_chunk_tokens + block['tokens'] > config.MAX_TOKENS and current_chunk_blocks:\n            # Create chunk\n            chunk_content = header + \"\\n\" + \"\\n\".join([b['content'] for b in current_chunk_blocks]) + \"\\n\" + footer\n            chunks.append((chunk_content, len(chunks) + 1, 0))\n            \n            # Start new chunk\n            current_chunk_blocks = [block]\n            current_chunk_tokens = estimate_token_count(header) + block['tokens']\n        else:\n            current_chunk_blocks.append(block)\n            current_chunk_tokens += block['tokens']\n    \n    # Add remaining blocks\n    if current_chunk_blocks:\n        chunk_content = header + \"\\n\" + \"\\n\".join([b['content'] for b in current_chunk_blocks]) + \"\\n\" + footer\n        chunks.append((chunk_content, len(chunks) + 1, 0))\n    \n    # Update chunk numbers\n    total_chunks = len(chunks)\n    chunks = [(content, idx, total_chunks) for content, idx, _ in chunks]\n    \n    logger.info(f\"Split large folder into {total_chunks} chunks\")\n    return chunks\n\n\ndef create_chunks_original(xml_content: str, file_name: str) -> List[Tuple[str, int, int]]:\n    \"\"\"\n    Original chunking logic as fallback\n    \"\"\"\n    sections = extract_xml_sections(xml_content)\n    chunks = []\n    current_chunk = \"\"\n    current_chunk_sections = []\n    \n    # Extract wrapper elements\n    powermart_header = \"\"\n    powermart_footer = \"</REPOSITORY>\\n</POWERMART>\"\n    \n    header_match = re.search(\n        r'(<\\?xml[^>]*>.*?<POWERMART[^>]*>.*?<REPOSITORY[^>]*>)',\n        xml_content,\n        re.DOTALL\n    )\n    if header_match:\n        powermart_header = header_match.group(1)\n    \n    for section_content, section_type in sections:\n        section_tokens = estimate_token_count(section_content)\n        current_tokens = estimate_token_count(current_chunk)\n        \n        if current_tokens + section_tokens > config.MAX_TOKENS and current_chunk:\n            wrapped_chunk = (\n                f\"{powermart_header}\\n\"\n                f\"<!-- Chunk containing: {', '.join(current_chunk_sections)} -->\\n\"\n                f\"{current_chunk}\\n\"\n                f\"{powermart_footer}\"\n            )\n            chunks.append((wrapped_chunk, len(chunks) + 1, 0))\n            current_chunk = \"\"\n            current_chunk_sections = []\n        \n        current_chunk += section_content + \"\\n\"\n        current_chunk_sections.append(section_type)\n        \n        if section_tokens > config.MAX_TOKENS:\n            logger.warning(\n                f\"Single {section_type} section exceeds token limit. \"\n                f\"Splitting by size...\"\n            )\n            section_parts = [\n                section_content[i:i+config.CHUNK_SIZE]\n                for i in range(0, len(section_content), config.CHUNK_SIZE)\n            ]\n            for part in section_parts:\n                wrapped_part = (\n                    f\"{powermart_header}\\n\"\n                    f\"<!-- Partial {section_type} chunk -->\\n\"\n                    f\"{part}\\n\"\n                    f\"{powermart_footer}\"\n                )\n                chunks.append((wrapped_part, len(chunks) + 1, 0))\n            current_chunk = \"\"\n            current_chunk_sections = []\n    \n    if current_chunk:\n        wrapped_chunk = (\n            f\"{powermart_header}\\n\"\n            f\"<!-- Chunk containing: {', '.join(current_chunk_sections)} -->\\n\"\n            f\"{current_chunk}\\n\"\n            f\"{powermart_footer}\"\n        )\n        chunks.append((wrapped_chunk, len(chunks) + 1, 0))\n    \n    total_chunks = len(chunks)\n    chunks = [(content, idx, total_chunks) for content, idx, _ in chunks]\n    \n    logger.info(f\"Created {total_chunks} section-based chunks for {file_name}\")\n    return chunks\n\n# ============================================================================\n# PROMPT GENERATION - ENHANCED VERSION\n# ============================================================================\n\ndef create_conversion_prompt(\n    xml_content: str,\n    file_name: str,\n    project_name: str = None,\n    chunk_info: Tuple[int, int] = None,\n    is_combined: bool = False\n) -> str:\n    \"\"\"\n    Enhanced prompt with explicit transformation-to-model mapping - GENERIC VERSION\n    \"\"\"\n    \n    chunk_context = \"\"\n    if chunk_info:\n        chunk_idx, total_chunks = chunk_info\n        chunk_context = (\n            f\"\\nNOTE: This is chunk {chunk_idx} of {total_chunks} \"\n            f\"from file {file_name}.\"\n        )\n    \n    project_context = \"\"\n    if project_name:\n        project_context = f\"\\nPROJECT NAME: {project_name}\"\n        if is_combined:\n            project_context += (\n                f\"\\nNOTE: This is part of a combined DBT project \"\n                f\"'{project_name}' containing multiple workflows.\"\n            )\n    \n    # Extract detailed mapping analysis\n    mapping_analysis = \"\"\n    transformation_flow = \"\"\n    mapping_sections = re.findall(r'<MAPPING\\s+[^>]*>.*?</MAPPING>', xml_content, re.DOTALL)\n    \n    if mapping_sections:\n        mapping_analysis = \"\\n\\n=== DETAILED MAPPING ANALYSIS ===\\n\"\n        for idx, mapping_xml in enumerate(mapping_sections[:3], 1):\n            details = parse_mapping_details(mapping_xml)\n            if details['mapping_name']:\n                mapping_analysis += f\"\\nMapping {idx}: {details['mapping_name']}\\n\"\n                mapping_analysis += f\"Transformations found: {len(details['transformations'])}\\n\"\n                \n                # List all transformations with types\n                if details['transformations']:\n                    mapping_analysis += \"Transformation breakdown:\\n\"\n                    for trans in details['transformations']:\n                        mapping_analysis += f\"  - {trans['name']} (Type: {trans['type']})\\n\"\n                \n                # Show complete data flow\n                if details['connectors']:\n                    mapping_analysis += f\"\\nData flow ({len(details['connectors'])} steps):\\n\"\n                    for conn in details['connectors']:\n                        mapping_analysis += f\"  {conn['from']} --> {conn['to']}\\n\"\n                    \n                    # Build transformation flow for the prompt\n                    transformation_flow = \"\\n\\n=== REQUIRED MODEL STRUCTURE ===\\n\"\n                    transformation_flow += \"Based on the CONNECTOR tags, you MUST create these models:\\n\\n\"\n                    \n                    # Identify each step in the flow\n                    for i, conn in enumerate(details['connectors'], 1):\n                        from_inst = conn['from']\n                        to_inst = conn['to']\n                        \n                        # Determine model type for each step\n                        if 'sq_' in from_inst.lower() or 'source' in from_inst.lower():\n                            transformation_flow += f\"Step {i}: Staging model for {from_inst}\\n\"\n                            transformation_flow += f\"  -> File: models/staging/stg_{from_inst.lower()}.sql\\n\\n\"\n                        \n                        if any(prefix in to_inst.lower() for prefix in ['exp_', 'expression', 'filter', 'router']):\n                            transformation_flow += f\"Step {i}: Intermediate model for transformation {to_inst}\\n\"\n                            transformation_flow += f\"  -> File: models/intermediate/int_{to_inst.lower()}.sql\\n\"\n                            transformation_flow += f\"  -> This model MUST reference the previous step using ref()\\n\\n\"\n                        \n                        elif any(prefix in to_inst.lower() for prefix in ['lkp_', 'lookup', 'jnr_', 'joiner']):\n                            transformation_flow += f\"Step {i}: Intermediate model for {to_inst}\\n\"\n                            transformation_flow += f\"  -> File: models/intermediate/int_{to_inst.lower()}.sql\\n\"\n                            transformation_flow += f\"  -> This model MUST implement the join/lookup logic\\n\\n\"\n                        \n                        elif any(prefix in to_inst.lower() for prefix in ['agg_', 'aggregator', 'rank', 'sorter']):\n                            transformation_flow += f\"Step {i}: Intermediate model for {to_inst}\\n\"\n                            transformation_flow += f\"  -> File: models/intermediate/int_{to_inst.lower()}.sql\\n\"\n                            transformation_flow += f\"  -> This model MUST implement aggregation/ranking\\n\\n\"\n                    \n                    # Final mart model\n                    last_connector = details['connectors'][-1] if details['connectors'] else None\n                    if last_connector:\n                        transformation_flow += f\"FINAL STEP: Mart model for target {last_connector['to']}\\n\"\n                        transformation_flow += f\"  -> File: models/marts/fct_{last_connector['to'].lower()}.sql or dim_{last_connector['to'].lower()}.sql\\n\"\n                        transformation_flow += f\"  -> This MUST be the final model in the chain\\n\"\n                        transformation_flow += f\"  -> It MUST reference the last intermediate model\\n\\n\"\n    \n    prompt = f\"\"\"\nYou are an expert data engineer specializing in migrating Informatica PowerCenter workflows to modern dbt projects.\n\nTASK: Convert the provided Informatica XML workflow into a complete, production-ready dbt project structure.\n\nINPUT FILE: {file_name}{project_context}{chunk_context}{mapping_analysis}{transformation_flow}\n\n=== CRITICAL REQUIREMENTS ===\n\n1. **EVERY TRANSFORMATION MUST BECOME A MODEL**\n   - Source Qualifier -> Staging model (stg_*)\n   - Expression -> Intermediate model (int_expression_*)\n   - Filter -> Intermediate model (int_filter_*)\n   - Lookup -> Intermediate model (int_lookup_*)\n   - Joiner -> Intermediate model (int_join_*)\n   - Aggregator -> Intermediate model (int_agg_*)\n   - Any other transformation -> Intermediate model (int_*)\n   - Target -> Mart model (fct_* or dim_*)\n\n2. **FOLLOW CONNECTOR FLOW EXACTLY**\n   - CONNECTOR tags show: A -> B -> C -> D\n   - Create models: stg_a -> int_b -> int_c -> fct_d\n   - Each model references the previous one using ref()\n   \n3. **MANDATORY OUTPUT FOR EVERY WORKFLOW**\n   - At least 1 staging model (for each SOURCE)\n   - At least 1 intermediate model (for each TRANSFORMATION)\n   - Exactly 1 mart model (for each TARGET)\n   - schema.yml files for ALL layers\n   - sources.yml for staging\n   - Macros for repeated logic\n   \n=== DETAILED CONVERSION RULES ===\n\n**STEP 1: Analyze XML Structure**\n1. Find all <SOURCE> tags -> create staging models\n2. Find all <TRANSFORMATION> tags -> create intermediate models\n3. Find all <CONNECTOR> tags -> determine model dependencies\n4. Find all <TARGET> tags -> create mart models\n\n**STEP 2: Source to Staging Mapping**\n\nFor EACH <SOURCE> in the XML:\n```xml\n<SOURCE NAME=\"[ANY_SOURCE_NAME]\" DBDNAME=\"[SCHEMA]\" OWNERNAME=\"[OWNER]\">\n  <SOURCEFIELD NAME=\"[FIELD1]\" DATATYPE=\"[TYPE1]\" KEYTYPE=\"PRIMARY KEY\"/>\n  <SOURCEFIELD NAME=\"[FIELD2]\" DATATYPE=\"[TYPE2]\"/>\n</SOURCE>\n```\n\nYou MUST create:\n\nFILE: models/staging/stg_[source_name_lowercase].sql\n```sql\n-- Staging: [SOURCE_NAME]\n-- Source: [SCHEMA].[SOURCE_NAME]\n-- Raw data from source system\n\nselect\n    [field1],\n    [field2],\n    [field3]\n    -- Include ALL fields from source\nfrom {{{{ source('[schema_name]', '[source_table_name]') }}}}\n```\n\n**STEP 3: Transformation to Intermediate Mapping**\n\nFor Source Qualifier transformation:\n```xml\n<TRANSFORMATION NAME=\"[SQ_NAME]\" TYPE=\"Source Qualifier\">\n```\n\nCreate:\nFILE: models/intermediate/int_[sq_name_lowercase].sql\n```sql\n-- Source Qualifier: [SQ_NAME]\n-- Applies source filtering and column selection\n\nselect \n    [column1],\n    [column2],\n    [column3]\n    -- Select only needed columns\nfrom {{{{ ref('stg_[source_name]') }}}}\nwhere [filter_conditions_if_any]\n```\n\nFor Expression transformation:\n```xml\n<TRANSFORMATION NAME=\"[EXP_NAME]\" TYPE=\"Expression\">\n  <TRANSFORMFIELD NAME=\"[OUTPUT_FIELD]\" EXPRESSION=\"[EXPRESSION_LOGIC]\"/>\n</TRANSFORMATION>\n```\n\nCreate:\nFILE: models/intermediate/int_[exp_name_lowercase].sql\n```sql\n-- Expression: [EXP_NAME]\n-- Applies business logic transformations\n\nselect\n    *,  -- Carry forward all existing columns\n    [expression_logic] as [output_field],\n    [another_expression] as [another_field]\n    -- Add calculated columns based on TRANSFORMFIELD expressions\nfrom {{{{ ref('[previous_model_name]') }}}}\n```\n\nFor Lookup transformation:\n```xml\n<TRANSFORMATION NAME=\"[LKP_NAME]\" TYPE=\"Lookup Procedure\">\n```\n\nCreate:\nFILE: models/intermediate/int_[lkp_name_lowercase].sql\n```sql\n-- Lookup: [LKP_NAME]\n-- Enriches data with lookup values\n\nselect\n    a.*,\n    b.[lookup_column1],\n    b.[lookup_column2]\nfrom {{{{ ref('[previous_model_name]') }}}} a\nleft join {{{{ ref('stg_[lookup_source]') }}}} b\n    on a.[join_key] = b.[lookup_key]\n```\n\nFor Filter transformation:\n```xml\n<TRANSFORMATION NAME=\"[FLT_NAME]\" TYPE=\"Filter\">\n```\n\nCreate:\nFILE: models/intermediate/int_[flt_name_lowercase].sql\n```sql\n-- Filter: [FLT_NAME]\n-- Filters rows based on conditions\n\nselect *\nfrom {{{{ ref('[previous_model_name]') }}}}\nwhere [filter_condition]\n```\n\nFor Joiner transformation:\n```xml\n<TRANSFORMATION NAME=\"[JNR_NAME]\" TYPE=\"Joiner\">\n```\n\nCreate:\nFILE: models/intermediate/int_[jnr_name_lowercase].sql\n```sql\n-- Joiner: [JNR_NAME]\n-- Joins multiple data streams\n\nselect\n    a.*,\n    b.[columns_from_second_stream]\nfrom {{{{ ref('[first_input_model]') }}}} a\n[join_type] join {{{{ ref('[second_input_model]') }}}} b\n    on a.[key] = b.[key]\n```\n\nFor Aggregator transformation:\n```xml\n<TRANSFORMATION NAME=\"[AGG_NAME]\" TYPE=\"Aggregator\">\n```\n\nCreate:\nFILE: models/intermediate/int_[agg_name_lowercase].sql\n```sql\n-- Aggregator: [AGG_NAME]\n-- Aggregates data\n\nselect\n    [group_by_columns],\n    count(*) as record_count,\n    sum([measure_column]) as total_[measure],\n    avg([measure_column]) as avg_[measure]\nfrom {{{{ ref('[previous_model_name]') }}}}\ngroup by [group_by_columns]\n```\n\n**STEP 4: Target to Mart Mapping**\n\nFor database table target:\n```xml\n<TARGET NAME=\"[TARGET_NAME]\" DATABASETYPE=\"Oracle\">\n</TARGET>\n```\n\nCreate:\nFILE: models/marts/fct_[target_name_lowercase].sql (for fact tables)\nOR\nFILE: models/marts/dim_[target_name_lowercase].sql (for dimension tables)\n\n```sql\n-- Mart: [TARGET_NAME]\n-- Final analytical data ready for consumption\n-- Data flow: [list all models in the chain]\n\n{{{{\n    config(\n        materialized='table',\n        tags=['[domain]', 'mart']\n    )\n}}}}\n\nselect\n    [all_final_columns],\n    current_timestamp() as loaded_at\nfrom {{{{ ref('[last_intermediate_model]') }}}}\n```\n\nFor flat file target:\n```xml\n<TARGET NAME=\"[FF_TARGET_NAME]\" DATABASETYPE=\"Flat File\">\n</TARGET>\n```\n\nCreate:\nFILE: models/marts/fct_[target_name_lowercase].sql\n```sql\n-- Mart: [FF_TARGET_NAME] (Flat file export)\n-- Prepares data for external file export\n-- Note: Actual file writing happens outside DBT\n\n{{{{\n    config(\n        materialized='table',\n        tags=['export', 'flatfile']\n    )\n}}}}\n\nselect *\nfrom {{{{ ref('[last_intermediate_model]') }}}}\n```\n\n**STEP 5: Create ALL Required YAML Files**\n\nFILE: dbt_project.yml\n```yaml\nname: '{project_name or file_name.replace(\".xml\", \"\").replace(\".XML\", \"\").lower()}'\nversion: '1.0.0'\nconfig-version: 2\n\nmodels:\n  {project_name or file_name.replace(\".xml\", \"\").replace(\".XML\", \"\").lower()}:\n    staging:\n      +materialized: view\n      +tags: ['staging']\n    intermediate:\n      +materialized: view\n      +tags: ['intermediate']\n    marts:\n      +materialized: table\n      +tags: ['mart']\n```\n\nFILE: models/staging/sources.yml\n```yaml\nversion: 2\n\nsources:\n  - name: [source_system_name]\n    description: \"Source system: [description]\"\n    database: [database_name]\n    schema: [schema_name]\n    tables:\n      - name: [table_name]\n        description: \"Source table: [table_name]\"\n        columns:\n          - name: [primary_key_column]\n            description: \"Primary key\"\n            tests:\n              - unique\n              - not_null\n          - name: [other_columns]\n            description: \"Column description\"\n```\n\nFILE: models/staging/schema.yml\n```yaml\nversion: 2\n\nmodels:\n  - name: stg_[table_name]\n    description: \"Staging model for [source_table_name]\"\n    columns:\n      - name: [primary_key]\n        description: \"Primary key column\"\n        tests:\n          - unique\n          - not_null\n      - name: [other_columns]\n        description: \"Column description\"\n```\n\nFILE: models/intermediate/schema.yml\n```yaml\nversion: 2\n\nmodels:\n  - name: int_[transformation_name]\n    description: \"Intermediate model: [transformation description]\"\n    columns:\n      - name: [key_columns]\n        description: \"Key column description\"\n        tests:\n          - not_null\n```\n\nFILE: models/marts/schema.yml\n```yaml\nversion: 2\n\nmodels:\n  - name: fct_[target_name]\n    description: \"Fact/Dim table: [target description]\"\n    columns:\n      - name: [primary_key]\n        description: \"Primary key\"\n        tests:\n          - unique\n          - not_null\n      - name: [measure_columns]\n        description: \"Measure/attribute description\"\n```\n\n**STEP 6: Create Macros for Reusable Logic**\n\nIf you see repeated transformation patterns (like date formatting, string manipulation), create macros:\n\nFILE: macros/[macro_name].sql\n```sql\n{{% macro [macro_name]([parameters]) %}}\n    -- Macro description: [what it does]\n    [SQL transformation logic]\n{{% endmacro %}}\n```\n\nThen use in models:\n```sql\nselect\n    {{{{ [macro_name]([column_name]) }}}} as [output_column]\nfrom {{{{ ref('[source_model]') }}}}\n```\n\n=== VALIDATION CHECKLIST ===\n\nBefore submitting, verify you created:\n☐ Staging model (stg_*.sql) for EVERY <SOURCE> tag\n☐ Intermediate model (int_*.sql) for EVERY <TRANSFORMATION> tag  \n☐ Mart model (fct_*.sql or dim_*.sql) for EVERY <TARGET> tag\n☐ sources.yml with ALL source tables defined\n☐ schema.yml in staging/ with ALL staging models documented\n☐ schema.yml in intermediate/ with ALL intermediate models documented\n☐ schema.yml in marts/ with ALL mart models documented\n☐ dbt_project.yml with correct project configuration\n☐ Macros for any repeated transformation logic\n☐ Model dependencies follow CONNECTOR flow exactly\n☐ Each model uses ref() to reference the correct previous model\n☐ Primary keys have unique and not_null tests\n\n=== COMMON TRANSFORMATION PATTERNS ===\n\n**Pattern 1: Simple Pass-Through**\nIf a transformation just passes data through, still create the model:\n```sql\nselect * from {{{{ ref('[previous_model]') }}}}\n```\n\n**Pattern 2: Column Renaming**\n```sql\nselect\n    [old_column_name] as [new_column_name],\n    [other_columns]\nfrom {{{{ ref('[previous_model]') }}}}\n```\n\n**Pattern 3: Type Casting**\n```sql\nselect\n    cast([column] as [new_type]) as [column],\n    [other_columns]\nfrom {{{{ ref('[previous_model]') }}}}\n```\n\n**Pattern 4: Conditional Logic**\n```sql\nselect\n    *,\n    case\n        when [condition1] then [value1]\n        when [condition2] then [value2]\n        else [default_value]\n    end as [new_column]\nfrom {{{{ ref('[previous_model]') }}}}\n```\n\n=== OUTPUT FORMAT ===\n\nStructure your response with clear file headers:\n\nFILE: dbt_project.yml\n```yaml\n[content]\n```\n\nFILE: models/staging/sources.yml\n```yaml\n[content]\n```\n\nFILE: models/staging/schema.yml\n```yaml\n[content]\n```\n\nFILE: models/staging/stg_[name].sql\n```sql\n[content]\n```\n\nFILE: models/intermediate/schema.yml\n```yaml\n[content]\n```\n\nFILE: models/intermediate/int_[name].sql\n```sql\n[content]\n```\n\nFILE: models/marts/schema.yml\n```yaml\n[content]\n```\n\nFILE: models/marts/fct_[name].sql\n```sql\n[content]\n```\n\nFILE: macros/[name].sql (if needed)\n```sql\n[content]\n```\n\n=== INFORMATICA XML WORKFLOW ===\n\n```xml\n{xml_content}\n```\n\n**FINAL REMINDERS:**\n1. Count TRANSFORMATION tags in XML = number of intermediate models you must create\n2. Count TARGET tags in XML = number of mart models you must create  \n3. Count SOURCE tags in XML = number of staging models you must create\n4. Create schema.yml for EVERY layer (staging, intermediate, marts)\n5. Follow CONNECTOR flow: each model refs the previous one in the chain\n6. Use generic names based on XML, not hardcoded names\n7. Every target MUST have a mart model - no exceptions!\n\nConvert this workflow to a complete dbt project NOW.\n\"\"\"\n    return prompt\n\n# ============================================================================\n# CLAUDE API CALLS\n# ============================================================================\n\n@rate_limit(calls_per_minute=config.RATE_LIMIT_CALLS_PER_MINUTE)\n@retry_on_failure(max_retries=config.MAX_RETRIES, delay=config.RETRY_DELAY)\ndef call_claude_api(prompt: str) -> str:\n    \"\"\"Call Claude API with rate limiting and retries\"\"\"\n    try:\n        logger.debug(f\"Calling Claude API with {len(prompt)} character prompt\")\n        response = complete(config.CLAUDE_MODEL, prompt)\n        \n        if not response:\n            raise ClaudeAPIError(\"Empty response from Claude API\")\n        \n        return response\n        \n    except Exception as e:\n        logger.error(f\"Claude API call failed: {e}\")\n        raise ClaudeAPIError(f\"Claude API error: {str(e)}\")\n\n# ============================================================================\n# VALIDATION AND PARSING\n# ============================================================================\n\ndef validate_dbt_sql(sql_content: str) -> Tuple[bool, Optional[str]]:\n    \"\"\"Validate DBT SQL content\"\"\"\n    try:\n        # Check for required DBT patterns\n        has_ref_or_source = bool(\n            re.search(r'{{\\s*(ref|source)\\s*\\(', sql_content)\n        )\n        if not has_ref_or_source:\n            return False, \"No ref() or source() found in SQL\"\n        \n        # Check for balanced Jinja braces\n        open_count = sql_content.count('{{')\n        close_count = sql_content.count('}}')\n        \n        if open_count != close_count:\n            return False, f\"Unbalanced Jinja braces: {open_count} {{ vs {close_count} }}\"\n        \n        return True, None\n    \n    except Exception as e:\n        return False, f\"Validation error: {str(e)}\"\n\ndef validate_yaml_content(yaml_content: str) -> Tuple[bool, Optional[str]]:\n    \"\"\"Validate YAML content structure\"\"\"\n    try:\n        # Basic YAML structure checks\n        if not yaml_content.strip():\n            return False, \"Empty YAML content\"\n        \n        # Check for common YAML errors\n        if yaml_content.count(':') == 0:\n            return False, \"No key-value pairs found\"\n        \n        # Check indentation consistency\n        lines = yaml_content.split('\\n')\n        indents = set()\n        for line in lines:\n            if line.strip() and not line.strip().startswith('#'):\n                indent = len(line) - len(line.lstrip())\n                if indent > 0:\n                    indents.add(indent)\n        \n        if indents and min(indents) not in [2, 4]:\n            return False, \"Inconsistent indentation (should be 2 or 4 spaces)\"\n        \n        return True, None\n    \n    except Exception as e:\n        return False, f\"Validation error: {str(e)}\"\n\ndef parse_claude_response(response: str) -> Dict[str, str]:\n    \"\"\"Parse Claude response to extract individual DBT files\"\"\"\n    files = {}\n    \n    try:\n        # Primary pattern: FILE: path\n        file_pattern = r'FILE:\\s*([^\\n]+)\\n```(?:sql|yaml|yml)?\\n(.*?)```'\n        matches = re.findall(file_pattern, response, re.DOTALL | re.MULTILINE)\n        \n        for file_path, content in matches:\n            clean_path = file_path.strip()\n            clean_content = content.strip()\n            \n            if clean_path and clean_content:\n                # Validate based on file type\n                if clean_path.endswith('.sql'):\n                    is_valid, error = validate_dbt_sql(clean_content)\n                    if not is_valid:\n                        logger.warning(\n                            f\"SQL validation failed for {clean_path}: {error}\"\n                        )\n                elif clean_path.endswith(('.yml', '.yaml')):\n                    is_valid, error = validate_yaml_content(clean_content)\n                    if not is_valid:\n                        logger.warning(\n                            f\"YAML validation failed for {clean_path}: {error}\"\n                        )\n                \n                files[clean_path] = clean_content\n        \n        # Alternative pattern if primary doesn't work\n        if not files:\n            alt_pattern = r'([^\\n]*\\.(?:sql|yml|yaml))\\n```(?:sql|yaml|yml)?\\n(.*?)```'\n            alt_matches = re.findall(alt_pattern, response, re.DOTALL)\n            \n            for file_path, content in alt_matches:\n                clean_path = file_path.strip()\n                clean_content = content.strip()\n                \n                if clean_path and clean_content:\n                    files[clean_path] = clean_content\n        \n        if not files:\n            logger.warning(\n                \"No structured files found in Claude response, saving raw response\"\n            )\n            files['claude_response.txt'] = response\n        \n        logger.info(f\"Parsed {len(files)} files from Claude response\")\n        return files\n        \n    except Exception as e:\n        logger.error(f\"Error parsing Claude response: {e}\")\n        return {'claude_response.txt': response}\n\ndef convert_xml_to_dbt(\n    session,\n    xml_content: str,\n    file_name: str,\n    project_name: str = None,\n    retry_count: int = 0,\n    is_combined: bool = False\n) -> Dict[str, Any]:\n    \"\"\"Convert XML to DBT using Claude with chunking support\"\"\"\n    try:\n        estimated_tokens = estimate_token_count(xml_content)\n        \n        if estimated_tokens > config.MAX_TOKENS:\n            logger.info(\n                f\"File {file_name} requires chunking \"\n                f\"({estimated_tokens:,} estimated tokens)\"\n            )\n            return convert_large_xml_to_dbt(\n                session, xml_content, file_name, project_name, is_combined\n            )\n        else:\n            logger.info(\n                f\"Converting {file_name} using {config.CLAUDE_MODEL} \"\n                f\"(attempt {retry_count + 1})\"\n            )\n            prompt = create_conversion_prompt(\n                xml_content, file_name, project_name, None, is_combined\n            )\n            \n            with track_time(f\"Claude conversion for {file_name}\"):\n                start_time = time.time()\n                response = call_claude_api(prompt)\n                conversion_time = time.time() - start_time\n            \n            dbt_files = parse_claude_response(response)\n            \n            if not dbt_files:\n                raise ConversionError(\"No DBT files generated from Claude response\")\n            \n            logger.info(f\"Generated {len(dbt_files)} DBT files for {file_name}\")\n            \n            return {\n                'status': 'success',\n                'files': dbt_files,\n                'processing_time': conversion_time,\n                'file_count': len(dbt_files),\n                'chunks_processed': 1\n            }\n    \n    except ClaudeAPIError as e:\n        if \"token\" in str(e).lower() and retry_count == 0:\n            logger.warning(\"Token limit exceeded, retrying with chunking\")\n            return convert_large_xml_to_dbt(\n                session, xml_content, file_name, project_name, is_combined\n            )\n        \n        error_msg = f\"Conversion failed: {str(e)}\"\n        logger.error(error_msg)\n        logger.debug(traceback.format_exc())\n        \n        return {\n            'status': 'failed',\n            'error': error_msg,\n            'processing_time': 0\n        }\n    \n    except Exception as e:\n        error_msg = f\"Unexpected error during conversion: {str(e)}\"\n        logger.error(error_msg)\n        logger.debug(traceback.format_exc())\n        \n        return {\n            'status': 'failed',\n            'error': error_msg,\n            'processing_time': 0\n        }\n\ndef convert_large_xml_to_dbt(\n    session,\n    xml_content: str,\n    file_name: str,\n    project_name: str = None,\n    is_combined: bool = False\n) -> Dict[str, Any]:\n    \"\"\"Convert large XML files using intelligent chunking\"\"\"\n    try:\n        logger.info(f\"Starting chunked conversion for {file_name}\")\n        \n        chunks = create_chunks(xml_content, file_name)\n        all_dbt_files = {}\n        total_time = 0\n        successful_chunks = 0\n        \n        for chunk_content, chunk_idx, total_chunks in chunks:\n            logger.info(\n                f\"Processing chunk {chunk_idx}/{total_chunks} of {file_name}\")\n            \n            prompt = create_conversion_prompt(\n                chunk_content, file_name, project_name,\n                (chunk_idx, total_chunks), is_combined\n            )\n            \n            try:\n                with track_time(f\"Chunk {chunk_idx}/{total_chunks}\"):\n                    start_time = time.time()\n                    response = call_claude_api(prompt)\n                    chunk_time = time.time() - start_time\n                    total_time += chunk_time\n                \n                chunk_files = parse_claude_response(response)\n                \n                for file_path, content in chunk_files.items():\n                    if file_path.endswith('.sql') and total_chunks > 1:\n                        base_name = file_path.rsplit('.', 1)[0]\n                        chunk_file_path = f\"{base_name}_chunk{chunk_idx}.sql\"\n                        all_dbt_files[chunk_file_path] = content\n                    elif file_path.endswith(('.yml', '.yaml')):\n                        if file_path in all_dbt_files:\n                            all_dbt_files[file_path] += (\n                                f\"\\n# --- Chunk {chunk_idx} ---\\n{content}\"\n                            )\n                        else:\n                            all_dbt_files[file_path] = content\n                    else:\n                        all_dbt_files[file_path] = content\n                \n                successful_chunks += 1\n                logger.info(f\"Chunk {chunk_idx} processed successfully\")\n                \n            except Exception as e:\n                logger.error(f\"Error processing chunk {chunk_idx}: {e}\")\n                continue\n            \n            if chunk_idx < total_chunks:\n                time.sleep(1)\n        \n        if not all_dbt_files:\n            raise ConversionError(\"No DBT files generated from any chunks\")\n        \n        if total_chunks > 1:\n            summary = f\"\"\"# Chunking Summary for {file_name}\n- Total chunks: {total_chunks}\n- Successful chunks: {successful_chunks}\n- Total processing time: {total_time:.2f} seconds\n- Files generated: {len(all_dbt_files)}\n\nNote: SQL files have been split by chunk. Review and merge as needed.\n\"\"\"\n            all_dbt_files['CHUNKING_NOTES.md'] = summary\n        \n        logger.info(\n            f\"Chunked conversion completed: {successful_chunks}/{total_chunks} \"\n            f\"chunks successful, {len(all_dbt_files)} files generated\"\n        )\n        \n        return {\n            'status': 'success',\n            'files': all_dbt_files,\n            'processing_time': total_time,\n            'file_count': len(all_dbt_files),\n            'chunks_processed': total_chunks\n        }\n        \n    except Exception as e:\n        error_msg = f\"Chunked conversion failed: {str(e)}\"\n        logger.error(error_msg)\n        logger.debug(traceback.format_exc())\n        \n        return {\n            'status': 'failed',\n            'error': error_msg,\n            'processing_time': 0\n        }\n\n# ============================================================================\n# FILE COMBINATION AND REFERENCE FIXING\n# ============================================================================\n\ndef fix_refs_in_content(\n    content: str,\n    workflow_prefix: str,\n    all_model_names: set\n) -> str:\n    \"\"\"Fix ref() statements to include workflow prefix or match existing model names\"\"\"\n    ref_pattern = r\"{{\\s*ref\\(['\\\"]([^'\\\"]+)['\\\"]\\)\\s*}}\"\n    \n    def replace_ref(match):\n        model_ref = match.group(1)\n        \n        if model_ref.startswith(workflow_prefix + '_'):\n            return match.group(0)\n        \n        prefixed_ref = f\"{workflow_prefix}_{model_ref}\"\n        \n        if prefixed_ref in all_model_names:\n            return f\"{{{{ ref('{prefixed_ref}') }}}}\"\n        \n        for model_name in all_model_names:\n            if model_name.endswith('_' + model_ref) or model_name == model_ref:\n                return f\"{{{{ ref('{model_name}') }}}}\"\n        \n        return f\"{{{{ ref('{prefixed_ref}') }}}}\"\n    \n    return re.sub(ref_pattern, replace_ref, content)\n\ndef combine_dbt_files_for_project(\n    all_files_list: List[Dict[str, str]],\n    project_name: str,\n    file_names: List[str]\n) -> Dict[str, str]:\n    \"\"\"Combine multiple DBT file sets into a single project with fixed references\"\"\"\n    combined_files = {}\n    model_mapping = {}\n    all_model_names = set()\n    \n    # First pass: collect all model names\n    for files, source_file in zip(all_files_list, file_names):\n        workflow_prefix = (\n            source_file.replace('.xml', '')\n            .replace('.XML', '')\n            .replace(' ', '_')\n            .lower()\n        )\n        \n        for file_path in files.keys():\n            if file_path.endswith('.sql') and file_path.startswith('models/'):\n                parts = file_path.split('/')\n                if len(parts) >= 3:\n                    model_name = parts[-1].replace('.sql', '')\n                    prefixed_name = f\"{workflow_prefix}_{model_name}\"\n                    all_model_names.add(prefixed_name)\n    \n    logger.debug(f\"Collected {len(all_model_names)} model names across all files\")\n    \n    # Second pass: process and combine files\n    for idx, (files, source_file) in enumerate(zip(all_files_list, file_names)):\n        workflow_prefix = (\n            source_file.replace('.xml', '')\n            .replace('.XML', '')\n            .replace(' ', '_')\n            .lower()\n        )\n        \n        for file_path, content in files.items():\n            if file_path == 'dbt_project.yml':\n                if 'dbt_project.yml' not in combined_files:\n                    updated_content = re.sub(\n                        r\"name:\\s*['\\\"]?\\w+['\\\"]?\",\n                        f\"name: '{project_name}'\",\n                        content\n                    )\n                    \n                    if idx == 0 and len(file_names) > 1:\n                        workflows_comment = (\n                            \"\\n# This dbt project combines multiple workflows:\\n\"\n                            + \"\\n\".join(\n                                f\"# - {name.replace('.xml', '').replace('.XML', '')}\"\n                                for name in file_names\n                            )\n                        )\n                        lines = updated_content.split('\\n')\n                        for i, line in enumerate(lines):\n                            if 'config-version:' in line:\n                                lines.insert(i + 1, workflows_comment)\n                                break\n                        updated_content = '\\n'.join(lines)\n                    \n                    combined_files[file_path] = updated_content\n                    \n            elif file_path.endswith('.sql'):\n                fixed_content = fix_refs_in_content(\n                    content, workflow_prefix, all_model_names\n                )\n                \n                if file_path.startswith('models/'):\n                    parts = file_path.split('/')\n                    if len(parts) >= 3:\n                        model_name = parts[-1]\n                        new_name = f\"{workflow_prefix}_{model_name}\"\n                        new_path = f\"{'/'.join(parts[:-1])}/{new_name}\"\n                        combined_files[new_path] = (\n                            f\"-- Source: {source_file}\\n{fixed_content}\"\n                        )\n                        model_mapping[new_path] = source_file\n                else:\n                    combined_files[file_path] = fixed_content\n                    \n            elif file_path.endswith(('.yml', '.yaml')):\n                if file_path == 'dbt_project.yml':\n                    pass\n                elif 'sources.yml' in file_path or 'schema.yml' in file_path:\n                    if file_path in combined_files:\n                        content_to_add = content\n                        if content.startswith('version:'):\n                            lines = content.split('\\n')\n                            content_to_add = '\\n'.join(lines[1:])\n                        combined_files[file_path] += (\n                            f\"\\n# --- From {source_file} ---\\n{content_to_add}\"\n                        )\n                    else:\n                        combined_files[file_path] = f\"# From {source_file}\\n{content}\"\n                else:\n                    if file_path in combined_files:\n                        combined_files[file_path] += (\n                            f\"\\n# --- From {source_file} ---\\n{content}\"\n                        )\n                    else:\n                        combined_files[file_path] = f\"# From {source_file}\\n{content}\"\n            \n            else:\n                if file_path in combined_files:\n                    combined_files[file_path] += (\n                        f\"\\n\\n--- From {source_file} ---\\n{content}\"\n                    )\n                else:\n                    combined_files[file_path] = content\n    \n    # Add project README\n    readme_content = f\"\"\"# DBT Project: {project_name}\n\n## Overview\nThis DBT project was automatically generated from Informatica PowerCenter XML workflows.\n\n## Source Files\nThis project combines the following workflows:\n{chr(10).join(f\"- {name}\" for name in file_names)}\n\n## Project Structure\n- `models/staging/`: Raw data staging models\n- `models/intermediate/`: Business logic transformations\n- `models/marts/`: Final analytical models\n\n## Model Mapping\nEach model is prefixed with its source workflow for clarity.\n\nGenerated: {datetime.now().isoformat()}\nConverter Version: Production v2.0\n\"\"\"\n    combined_files['README.md'] = readme_content\n    \n    logger.info(\n        f\"Combined {len(all_files_list)} file sets into {len(combined_files)} files\")\n    \n    return combined_files\n\n# ============================================================================\n# PERSISTENCE FUNCTIONS\n# ============================================================================\n\n@retry_on_failure(max_retries=3, delay=2)\ndef save_dbt_files(\n    session,\n    dbt_files: Dict[str, str],\n    workflow_name: str,\n    project_name: str = None\n) -> bool:\n    \"\"\"Save DBT files to Snowflake table with project information\"\"\"\n    try:\n        effective_project = project_name or workflow_name\n        \n        file_records = []\n        \n        for file_path, content in dbt_files.items():\n            record = {\n                'project_name': effective_project,\n                'workflow_name': workflow_name,\n                'file_path': file_path,\n                'file_name': file_path.split('/')[-1],\n                'file_type': file_path.split('.')[-1] if '.' in file_path else 'txt',\n                'content': content,\n                'content_length': len(content),\n                'created_timestamp': datetime.now().isoformat()\n            }\n            file_records.append(record)\n        \n        df = session.create_dataframe(file_records)\n        \n        with track_time(f\"Saving {len(dbt_files)} files to Snowflake\"):\n            df.write.mode(\"append\").save_as_table(config.OUTPUT_TABLE)\n        \n        logger.info(\n            f\"Saved {len(dbt_files)} files to {config.OUTPUT_TABLE} \"\n            f\"for project '{effective_project}'\"\n        )\n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error saving DBT files: {e}\")\n        logger.debug(traceback.format_exc())\n        return False\n\ndef create_summary_record(\n    file_name: str,\n    workflow_name: str,\n    project_name: str,\n    result: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Create summary record for conversion result\"\"\"\n    return {\n        'source_file': file_name,\n        'project_name': project_name,\n        'workflow_name': workflow_name,\n        'conversion_status': result['status'],\n        'files_generated': int(result.get('file_count', 0)),\n        'chunks_processed': int(result.get('chunks_processed', 1)),\n        'processing_time_seconds': float(result.get('processing_time', 0.0)),\n        'error_message': result.get('error', None),\n        'converted_timestamp': datetime.now().isoformat()\n    }\n\ndef save_conversion_summary(session, results: List[Dict[str, Any]]):\n    \"\"\"Save conversion summary to Snowflake\"\"\"\n    try:\n        if not results:\n            logger.warning(\"No results to save in summary\")\n            return\n        \n        df = session.create_dataframe(results)\n        \n        with track_time(\"Saving conversion summary\"):\n            df.write.mode(\"overwrite\").save_as_table(config.SUMMARY_TABLE)\n        \n        logger.info(f\"Conversion summary saved to {config.SUMMARY_TABLE}\")\n        \n    except Exception as e:\n        logger.error(f\"Error saving summary: {e}\")\n        logger.debug(traceback.format_exc())\n\n# ============================================================================\n# BATCH PROCESSING\n# ============================================================================\n\ndef process_folder_group(\n    session,\n    folder_name: str,\n    file_paths: List[str]\n) -> List[Dict[str, Any]]:\n    \"\"\"Process a group of files from the same folder as one DBT project\"\"\"\n    results = []\n    \n    project_name = f\"dbt_{folder_name.lower().replace(' ', '_').replace('-', '_')}\"\n    \n    logger.info(f\"\\n{'='*60}\")\n    logger.info(f\"Processing folder: {folder_name}\")\n    logger.info(f\"Project name: {project_name}\")\n    logger.info(f\"Files in folder: {len(file_paths)}\")\n    logger.info(f\"{'='*60}\")\n    \n    if len(file_paths) == 1:\n        file_path = file_paths[0]\n        file_name = file_path.split('/')[-1]\n        \n        logger.info(f\"Processing single file: {file_name}\")\n        \n        try:\n            xml_content = read_xml_content(session, config.INPUT_STAGE, file_path)\n            result = convert_xml_to_dbt(\n                session, xml_content, file_name, project_name, is_combined=False\n            )\n            \n            if result['status'] == 'success':\n                workflow_name = (\n                    file_name.replace('.xml', '')\n                    .replace('.XML', '')\n                    .replace(' ', '_')\n                    .lower()\n                )\n                save_success = save_dbt_files(\n                    session, result['files'], workflow_name, project_name\n                )\n                \n                if save_success:\n                    logger.info(\n                        f\"Successfully saved {result['file_count']} DBT files \"\n                        f\"for project '{project_name}'\"\n                    )\n                else:\n                    result['status'] = 'partial_success'\n                    result['error'] = 'Generated files but failed to save'\n                    \n        except XMLReadError as e:\n            result = {\n                'status': 'failed',\n                'error': str(e),\n                'processing_time': 0\n            }\n        except Exception as e:\n            result = {\n                'status': 'failed',\n                'error': f\"Unexpected error: {str(e)}\",\n                'processing_time': 0\n            }\n        \n        workflow_name = (\n            file_name.replace('.xml', '')\n            .replace('.XML', '')\n            .replace(' ', '_')\n            .lower()\n        )\n        summary = create_summary_record(file_name, workflow_name, project_name, result)\n        results.append(summary)\n        \n    else:\n        logger.info(f\"Combining {len(file_paths)} files into single DBT project\")\n        \n        all_files = []\n        all_file_names = []\n        combined_time = 0\n        \n        for file_path in file_paths:\n            file_name = file_path.split('/')[-1]\n            logger.info(f\"Processing: {file_name}\")\n            \n            try:\n                xml_content = read_xml_content(session, config.INPUT_STAGE, file_path)\n                result = convert_xml_to_dbt(\n                    session, xml_content, file_name, project_name, is_combined=True\n                )\n                \n                workflow_name = (\n                    file_name.replace('.xml', '')\n                    .replace('.XML', '')\n                    .replace(' ', '_')\n                    .lower()\n                )\n                \n                if result['status'] == 'success':\n                    all_files.append(result['files'])\n                    all_file_names.append(file_name)\n                    combined_time += result.get('processing_time', 0)\n                \n                summary = create_summary_record(\n                    file_name, workflow_name, project_name, result\n                )\n                results.append(summary)\n                \n            except Exception as e:\n                logger.error(f\"Failed to process {file_name}: {e}\")\n                workflow_name = (\n                    file_name.replace('.xml', '')\n                    .replace('.XML', ''\n)\n                    .replace(' ', '_')\n                    .lower()\n                )\n                result = {\n                    'status': 'failed',\n                    'error': str(e),\n                    'processing_time': 0\n                }\n                summary = create_summary_record(\n                    file_name, workflow_name, project_name, result\n                )\n                results.append(summary)\n        \n        if all_files:\n            logger.info(\n                f\"Combining {len(all_files)} successful conversions \"\n                f\"into project '{project_name}'\"\n            )\n            combined_files = combine_dbt_files_for_project(\n                all_files, project_name, all_file_names\n            )\n            \n            save_success = save_dbt_files(\n                session, combined_files, f\"{project_name}_combined\", project_name\n            )\n            \n            if save_success:\n                logger.info(\n                    f\"Successfully created combined DBT project '{project_name}' \"\n                    f\"with {len(combined_files)} files\"\n                )\n    \n    return results\n\ndef process_files_batch(\n    session,\n    folder_files: Dict[str, List[str]]\n) -> List[Dict[str, Any]]:\n    \"\"\"Process XML files grouped by folder\"\"\"\n    all_results = []\n    total_folders = len(folder_files)\n    \n    logger.info(f\"\\nStarting batch processing of {total_folders} folders\")\n    logger.info(f\"Model: {config.CLAUDE_MODEL}, Max Tokens: {config.MAX_TOKENS:,}\")\n    \n    for folder_idx, (folder_name, file_paths) in enumerate(folder_files.items(), 1):\n        logger.info(\n            f\"\\n[Folder {folder_idx}/{total_folders}] Processing: {folder_name}\")\n        \n        try:\n            folder_results = process_folder_group(session, folder_name, file_paths)\n            all_results.extend(folder_results)\n        except Exception as e:\n            logger.error(f\"Error processing folder {folder_name}: {e}\")\n            logger.debug(traceback.format_exc())\n        \n        if folder_idx < total_folders:\n            logger.debug(\"Pausing between folders\")\n            time.sleep(2)\n    \n    return all_results\n\n# ============================================================================\n# REPORTING\n# ============================================================================\n\ndef print_final_report(results: List[Dict[str, Any]]):\n    \"\"\"Print comprehensive final conversion report\"\"\"\n    successful = [r for r in results if r['conversion_status'] == 'success']\n    failed = [r for r in results if r['conversion_status'] == 'failed']\n    \n    projects = {}\n    for r in results:\n        project = r.get('project_name', 'unknown')\n        if project not in projects:\n            projects[project] = {\n                'success': 0,\n                'failed': 0,\n                'files': 0,\n                'chunks': 0,\n                'time': 0\n            }\n        \n        if r['conversion_status'] == 'success':\n            projects[project]['success'] += 1\n            projects[project]['files'] += r.get('files_generated', 0)\n            projects[project]['chunks'] += r.get('chunks_processed', 1)\n        else:\n            projects[project]['failed'] += 1\n        \n        projects[project]['time'] += r.get('processing_time_seconds', 0)\n    \n    total_time = sum(r['processing_time_seconds'] for r in results)\n    total_files = sum(r['files_generated'] for r in successful)\n    total_chunks = sum(r.get('chunks_processed', 1) for r in results)\n    \n    logger.info(\"\\n\" + \"=\"*80)\n    logger.info(\"INFORMATICA TO DBT CONVERSION - FINAL REPORT\")\n    logger.info(\"=\"*80)\n    logger.info(f\"Total XML Files Processed: {len(results)}\")\n    logger.info(f\"Total DBT Projects Created: {len(projects)}\")\n    logger.info(f\"Successful Conversions: {len(successful)}\")\n    logger.info(f\"Failed Conversions: {len(failed)}\")\n    logger.info(\n        f\"Success Rate: {len(successful)/len(results)*100:.1f}%\"\n        if results else \"N/A\"\n    )\n    logger.info(f\"Total DBT Files Generated: {total_files}\")\n    logger.info(f\"Total Chunks Processed: {total_chunks}\")\n    logger.info(f\"Total Processing Time: {total_time:.2f} seconds\")\n    logger.info(\n        f\"Average Time per File: {total_time/len(results):.2f} seconds\"\n        if results else \"N/A\"\n    )\n    \n    logger.info(\"\\nProject Summary:\")\n    logger.info(\"-\" * 60)\n    for project_name, stats in projects.items():\n        logger.info(f\"\\nProject: {project_name}\")\n        logger.info(\n            f\"  • Workflows: {stats['success'] + stats['failed']} \"\n            f\"({stats['success']} successful, {stats['failed']} failed)\"\n        )\n        logger.info(f\"  • Files generated: {stats['files']}\")\n        logger.info(f\"  • Chunks processed: {stats['chunks']}\")\n        logger.info(f\"  • Processing time: {stats['time']:.2f} seconds\")\n    \n    if successful:\n        logger.info(\"\\nSuccessfully Converted Workflows:\")\n        for r in successful:\n            chunks_info = (\n                f\" ({r.get('chunks_processed', 1)} chunks)\"\n                if r.get('chunks_processed', 1) > 1 else \"\"\n            )\n            logger.info(\n                f\"  • {r['workflow_name']} → {r['project_name']} \"\n                f\"({r['files_generated']} files{chunks_info})\"\n            )\n    \n    if failed:\n        logger.info(\"\\nFailed Conversions:\")\n        for r in failed:\n            logger.info(f\"  • {r['source_file']}: {r['error_message']}\")\n    \n    logger.info(\"\\nNext Steps:\")\n    logger.info(f\"1. Review generated files in table: {config.OUTPUT_TABLE}\")\n    logger.info(\n        \"2. Query by project_name to get all files for a specific DBT project\")\n    logger.info(\"3. Download and test DBT projects in your development environment\")\n    logger.info(\"4. For chunked files, review and merge SQL models as needed\")\n    logger.info(\"5. Validate SQL logic and add custom business rules\")\n    logger.info(\"6. Deploy to production DBT environment\")\n    logger.info(\"=\"*80)\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    logger.info(\"=\"*80)\n    logger.info(\"Informatica PowerCenter XML to DBT Conversion Script\")\n    logger.info(\"Production Version v2.0\")\n    logger.info(\"=\"*80)\n    logger.info(f\"Using {config.CLAUDE_MODEL} via Snowflake Cortex\")\n    logger.info(\n        \"Enhanced with explicit transformation mapping and improved prompting\")\n    \n    logger.info(f\"\\nConfiguration:\")\n    logger.info(f\"  Input Stage: {config.INPUT_STAGE}\")\n    logger.info(f\"  Output Table: {config.OUTPUT_TABLE}\")\n    logger.info(f\"  Summary Table: {config.SUMMARY_TABLE}\")\n    logger.info(f\"  Claude Model: {config.CLAUDE_MODEL}\")\n    logger.info(f\"  Max Tokens: {config.MAX_TOKENS:,}\")\n    logger.info(f\"  Chunk Size: {config.CHUNK_SIZE:,} characters\")\n    logger.info(f\"  Batch Size: {config.BATCH_SIZE}\")\n    logger.info(f\"  Max Retries: {config.MAX_RETRIES}\")\n    logger.info(f\"  Rate Limit: {config.RATE_LIMIT_CALLS_PER_MINUTE} calls/minute\")\n    \n    try:\n        session = get_snowflake_session()\n        \n        folder_files = list_xml_files_with_folders(session, config.INPUT_STAGE)\n        \n        if not folder_files:\n            logger.warning(f\"No XML files found in {config.INPUT_STAGE}\")\n            logger.info(\n                \"Please upload your Informatica XML files to the stage and try again.\")\n            return\n        \n        total_files = sum(len(files) for files in folder_files.values())\n        \n        logger.info(\n            f\"\\nStarting conversion of {total_files} XML files \"\n            f\"in {len(folder_files)} folders\"\n        )\n        \n        with track_time(\"Total conversion process\"):\n            start_time = time.time()\n            results = process_files_batch(session, folder_files)\n            total_time = time.time() - start_time\n        \n        save_conversion_summary(session, results)\n        print_final_report(results)\n        \n        logger.info(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n        logger.info(\"Conversion process completed!\")\n        \n        logger.info(\"\\nTo retrieve DBT project files:\")\n        logger.info(\"-- Get all projects:\")\n        logger.info(f\"SELECT DISTINCT project_name FROM {config.OUTPUT_TABLE};\")\n        logger.info(\"\\n-- Get files for specific project:\")\n        logger.info(f\"SELECT * FROM {config.OUTPUT_TABLE}\")\n        logger.info(\"WHERE project_name = 'your_project_name' ORDER BY file_path;\")\n        \n    except KeyboardInterrupt:\n        logger.warning(\"\\nProcess interrupted by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {e}\")\n        logger.debug(traceback.format_exc())\n        raise\n\nif __name__ == \"__main__\":\n    main()",
   "execution_count": null,
   "outputs": []
  }
 ]
}